<div align="center">

# {{ .ADL.Metadata.Name | title }}

{{- if .GenerateCI }}
{{- if and .ADL.Spec.SCM (eq .ADL.Spec.SCM.Provider "github") .ADL.Spec.SCM.URL }}
{{- $url := .ADL.Spec.SCM.URL | trimSuffix ".git" | trimPrefix "https://github.com/" | trimPrefix "git@github.com:" }}
[![CI](https://github.com/{{ $url }}/workflows/CI/badge.svg)](https://github.com/{{ $url }}/actions/workflows/ci.yml)
{{- end }}
{{- end }}
{{- if eq .Language "go" }}
[![Go Version](https://img.shields.io/badge/Go-{{ .ADL.Spec.Language.Go.Version }}+-00ADD8?style=flat&logo=go)](https://golang.org)
{{- else if eq .Language "rust" }}
[![Rust Version](https://img.shields.io/badge/Rust-{{ .ADL.Spec.Language.Rust.Version }}+-000000?style=flat&logo=rust)](https://rust-lang.org)
{{- else if eq .Language "typescript" }}
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6?style=flat&logo=typescript)](https://typescriptlang.org)
{{- end }}
[![A2A Protocol](https://img.shields.io/badge/A2A-Protocol-blue?style=flat)](https://github.com/inference-gateway/adk)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**{{ .ADL.Metadata.Description }}**

A production-ready [Agent-to-Agent (A2A)](https://github.com/inference-gateway/adk) server that provides AI-powered capabilities through a standardized protocol.

</div>

## Quick Start

```bash
# Run the agent
{{- if eq .Language "go" }}
go run .
{{- else if eq .Language "rust" }}
cargo run
{{- else if eq .Language "typescript" }}
npm install
npm run dev
{{- end }}

# Or with Docker
docker build -t {{ .ADL.Metadata.Name }} .
docker run -p {{ .ADL.Spec.Server.Port | default 8080 }}:{{ .ADL.Spec.Server.Port | default 8080 }} {{ .ADL.Metadata.Name }}
```

## Features

- ✅ A2A protocol compliant
- ✅ AI-powered capabilities
{{- if .ADL.Spec.Capabilities }}
{{- if .ADL.Spec.Capabilities.Streaming }}
- ✅ Streaming support
{{- end }}
{{- if .ADL.Spec.Capabilities.PushNotifications }}
- ✅ Push notifications
{{- end }}
{{- if .ADL.Spec.Capabilities.StateTransitionHistory }}
- ✅ State transition history
{{- end }}
{{- end }}
- ✅ Production ready
- ✅ Minimal dependencies

## Endpoints

- `GET /.well-known/agent.json` - Agent metadata and capabilities
- `GET /health` - Health check endpoint
- `POST /a2a` - A2A protocol endpoint

## Available Skills

| Skill | Description | Parameters |
|-------|-------------|------------|
{{- range .ADL.Spec.Skills }}
| `{{ .Name }}` | {{ .Description }} | {{- if .Schema.properties }}{{- $props := list }}{{- range $key, $value := .Schema.properties }}{{- $props = append $props $key }}{{- end }}{{ $props | join ", " }}{{- else }}None{{- end }} |
{{- end }}

## Configuration

Configure the agent via environment variables (see `.env.example` for a complete template):

| Category | Variable | Description | Default |
|----------|----------|-------------|---------|
| **Server** | `A2A_PORT` | Server port | `{{ .ADL.Spec.Server.Port | default 8080 }}` |
| **Server** | `A2A_DEBUG` | Enable debug mode | `{{ .ADL.Spec.Server.Debug | default false }}` |
| **Server** | `A2A_AGENT_URL` | Agent URL for internal references | `http://localhost:{{ .ADL.Spec.Server.Port | default 8080 }}` |
| **Server** | `A2A_STREAMING_STATUS_UPDATE_INTERVAL` | Streaming status update frequency | `1s` |
| **Agent Metadata** | `A2A_AGENT_CARD_FILE_PATH` | Path to agent card JSON file | `.well-known/agent.json` |
| **LLM Client** | `A2A_AGENT_CLIENT_PROVIDER` | LLM provider (`openai`, `anthropic`, `azure`, `ollama`, `deepseek`) | {{- if .ADL.Spec.Agent }}`{{ .ADL.Spec.Agent.Provider }}`{{- else }}`openai`{{- end }} |
| **LLM Client** | `A2A_AGENT_CLIENT_MODEL` | Model to use | {{- if .ADL.Spec.Agent }}`{{ .ADL.Spec.Agent.Model }}`{{- else }}`gpt-4o-mini`{{- end }} |
| **LLM Client** | `A2A_AGENT_CLIENT_API_KEY` | API key for LLM provider | - |
| **LLM Client** | `A2A_AGENT_CLIENT_BASE_URL` | Custom LLM API endpoint | - |
| **LLM Client** | `A2A_AGENT_CLIENT_TIMEOUT` | Timeout for LLM requests | `30s` |
| **LLM Client** | `A2A_AGENT_CLIENT_MAX_RETRIES` | Maximum retries for LLM requests | `3` |
| **LLM Client** | `A2A_AGENT_CLIENT_MAX_CHAT_COMPLETION_ITERATIONS` | Max chat completion rounds | `10` |
| **LLM Client** | `A2A_AGENT_CLIENT_MAX_TOKENS` | Maximum tokens for LLM responses | {{- if and .ADL.Spec.Agent .ADL.Spec.Agent.MaxTokens }}`{{ .ADL.Spec.Agent.MaxTokens }}`{{- else }}`4096`{{- end }} |
| **LLM Client** | `A2A_AGENT_CLIENT_TEMPERATURE` | Controls randomness of LLM output | {{- if and .ADL.Spec.Agent .ADL.Spec.Agent.Temperature }}`{{ .ADL.Spec.Agent.Temperature }}`{{- else }}`0.7`{{- end }} |
| **Capabilities** | `A2A_CAPABILITIES_STREAMING` | Enable streaming responses | `{{ .ADL.Spec.Capabilities.Streaming | default true }}` |
| **Capabilities** | `A2A_CAPABILITIES_PUSH_NOTIFICATIONS` | Enable push notifications | `{{ .ADL.Spec.Capabilities.PushNotifications | default false }}` |
| **Capabilities** | `A2A_CAPABILITIES_STATE_TRANSITION_HISTORY` | Track state transitions | `{{ .ADL.Spec.Capabilities.StateTransitionHistory | default false }}` |
| **Task Management** | `A2A_TASK_RETENTION_MAX_COMPLETED_TASKS` | Max completed tasks to keep (0 = unlimited) | `100` |
| **Task Management** | `A2A_TASK_RETENTION_MAX_FAILED_TASKS` | Max failed tasks to keep (0 = unlimited) | `50` |
| **Task Management** | `A2A_TASK_RETENTION_CLEANUP_INTERVAL` | Cleanup frequency (0 = manual only) | `5m` |
| **Storage** | `A2A_QUEUE_PROVIDER` | Storage backend (`memory` or `redis`) | `memory` |
| **Storage** | `A2A_QUEUE_URL` | Redis connection URL (when using Redis) | - |
| **Storage** | `A2A_QUEUE_MAX_SIZE` | Maximum queue size | `100` |
| **Storage** | `A2A_QUEUE_CLEANUP_INTERVAL` | Task cleanup interval | `30s` |
{{- if and .ADL.Spec.Server.Auth .ADL.Spec.Server.Auth.Enabled }}
| **Authentication** | `A2A_AUTH_ENABLE` | Enable OIDC authentication | `true` |
| **Authentication** | `A2A_AUTH_ISSUER_URL` | OIDC issuer URL | - |
| **Authentication** | `A2A_AUTH_CLIENT_ID` | OIDC client ID | - |
| **Authentication** | `A2A_AUTH_CLIENT_SECRET` | OIDC client secret | - |
{{- else }}
| **Authentication** | `A2A_AUTH_ENABLE` | Enable OIDC authentication | `false` |
{{- end }}

## Development

```bash
# Generate code from ADL
task generate

# Run tests
task test

# Build the application
task build

# Run linter
task lint

# Format code
task fmt
```

### Debugging

Use the [A2A Debugger](https://github.com/inference-gateway/a2a-debugger) to test and debug your A2A agent during development. It provides a web interface for sending requests to your agent and inspecting responses, making it easier to troubleshoot issues and validate your implementation.

```bash
docker run --rm -it --network host ghcr.io/inference-gateway/a2a-debugger:latest --server-url http://localhost:8080 tasks submit "What are your skills?"
```

```bash
docker run --rm -it --network host ghcr.io/inference-gateway/a2a-debugger:latest --server-url http://localhost:8080 tasks list
```

```bash
docker run --rm -it --network host ghcr.io/inference-gateway/a2a-debugger:latest --server-url http://localhost:8080 tasks get <task ID>
```

## Deployment

### Docker

The Docker image can be built with custom version information using build arguments:

```bash
# Build with default values from ADL
docker build -t {{ .ADL.Metadata.Name }} .

# Build with custom version information
docker build \
  --build-arg VERSION=1.2.3 \
  --build-arg AGENT_NAME="My Custom Agent" \
  --build-arg AGENT_DESCRIPTION="Custom agent description" \
  -t {{ .ADL.Metadata.Name }}:1.2.3 .
```

**Available Build Arguments:**
- `VERSION` - Agent version (default: `{{ .ADL.Metadata.Version }}`)
- `AGENT_NAME` - Agent name (default: `{{ .ADL.Metadata.Name }}`)
- `AGENT_DESCRIPTION` - Agent description (default: `{{ .ADL.Metadata.Description }}`)

These values are embedded into the binary at build time using linker flags, making them accessible at runtime without requiring environment variables.

### Kubernetes

```bash
kubectl apply -f k8s/
```

## License

MIT License - see LICENSE file for details
